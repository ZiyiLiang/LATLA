plot(beta, (y - beta)^2 + lambda * beta^2, pch = 20)
beta_hat <- y / (1 + lambda)
points(beta_hat, (y - beta_hat)^2 + lambda * beta_hat^2, col = "red",lwd = 5)
y <- 3
lambda <- 3
beta <- seq(-10, 10, 0.1)
plot(beta, (y - beta)^2 + lambda * abs(beta), pch = 20, xlab = "beta", ylab = "Lasso optimization")
beta_hat <- y - lambda / 2
points(beta.est, (y - beta.est)^2 + lambda * abs(beta.est), col = "red", lwd = 5)
y <- 3
lambda <- 3
beta <- seq(-10, 10, 0.1)
plot(beta, (y - beta)^2 + lambda * abs(beta), pch = 20, xlab = "beta", ylab = "Lasso optimization")
beta_est <- y - lambda / 2
points(beta.est, (y - beta.est)^2 + lambda * abs(beta.est), col = "red", lwd = 5)
y <- 3
lambda <- 3
beta <- seq(-10, 10, 0.1)
plot(beta, (y - beta)^2 + lambda * abs(beta), pch = 20, xlab = "beta", ylab = "Lasso optimization")
beta.est <- y - lambda / 2
points(beta.est, (y - beta.est)^2 + lambda * abs(beta.est), col = "red", lwd = 5)
y <- 3
lambda <- 3
beta <- seq(-8, 8, 0.01)
plot(beta, (y - beta)^2 + lambda * abs(beta), pch = 20, xlab = "beta", ylab = "Lasso optimization")
beta.est <- y - lambda / 2
points(beta.est, (y - beta.est)^2 + lambda * abs(beta.est), col = "red", lwd = 5)
library(ISLR2)
library(ISLR)
install.packages("ISLR2")
library(ISLR2)
attach(College)
set.seed(2021)
subset<-sample(nrow(College),nrow(College)*0.8)
train<-College[subset,]
test<-College[-subset,]
ls<-lm(Apps~.,data=train)
summary(ls)
predicted.apps<-predict(ls,test)
testerror<-mean((test$Apps-predicted.apps)^2)
testerror
library(glmnet)
installed.packages("glmnet")
library(glmnet)
install.packages("glmnet")
train.mat<-model.matrix(Apps~.,data=train)
test.mat<-model.matrix(Apps~.,data=test)
grid<-10^seq(4,-2,length=100)
ridge<-glmnet(train.mat,train$Apps,alpha=0,lambda=grid,thresh = 1e-12)
cv.ridge<-cv.glmnet(train.mat,train$Apps,alpha=0,lambda=grid,thresh=1e-12)
bestlam.ridge<-cv.ridge$lambda.min
bestlam.ridge
library(glmnet)
train.mat<-model.matrix(Apps~.,data=train)
test.mat<-model.matrix(Apps~.,data=test)
grid<-10^seq(4,-2,length=100)
ridge<-glmnet(train.mat,train$Apps,alpha=0,lambda=grid,thresh = 1e-12)
cv.ridge<-cv.glmnet(train.mat,train$Apps,alpha=0,lambda=grid,thresh=1e-12)
bestlam.ridge<-cv.ridge$lambda.min
bestlam.ridge
train.mat<-model.matrix(Apps~.,data=train)
test.mat<-model.matrix(Apps~.,data=test)
grid<-10^seq(4,-2,length=100)
ridge<-glmnet(train.mat,train$Apps,alpha=0,lambda=grid,thresh = 1e-12)
cv.ridge<-cv.glmnet(train.mat,train$Apps,alpha=0,lambda=grid,thresh=1e-12)
bestlam.ridge<-cv.ridge$lambda.min
bestlam.ridge
pred.ridge<-predict(ridge,s=bestlam.ridge,newx =test.mat)
mean((test$Apps-pred.ridge)^2)
lasso<-glmnet(train.mat,train$Apps,alpha=1,lambda=grid,thresh = 1e-12)
cv.lasso<-cv.glmnet(train.mat,train$Apps,alpha=1,lambda=grid,thresh=1e-12)
bestlam.lasso<-cv.lasso$lambda.min
pred.lasso<-predict(lasso,s=bestlam.lasso,newx =test.mat)
mean((test$Apps-pred.lasso)^2)
library(pls)
pcrmodel<-pcr(Apps~.,data=train,scale=TRUE,validation="CV")
validationplot(pcrmodel,val.type="MSEP")
predict.pcr<-predict(pcrmodel,test,ncomp=17)
mean((test$Apps-predict.pcr)^2)
install.packages('pls')
library(pls)
pcrmodel<-pcr(Apps~.,data=train,scale=TRUE,validation="CV")
validationplot(pcrmodel,val.type="MSEP")
predict.pcr<-predict(pcrmodel,test,ncomp=17)
mean((test$Apps-predict.pcr)^2)
plsrmodel<-plsr(Apps~.,data=train,scale=TRUE,validation="CV")
validationplot(plsrmodel,val.type="MSEP")
predict.plsr<-predict(plsrmodel,test,ncomp=10)
mean((test$Apps-predict.plsr)^2)
plsrmodel<-plsr(Apps~.,data=train,scale=TRUE,validation="CV")
validationplot(plsrmodel,val.type="MSEP")
predict.plsr<-predict(plsrmodel,test,ncomp=10)
mean((test$Apps-predict.plsr)^2)
install.packages('MASS')
attach(Boston)
Test train split
set.seed(2021)
subset<-sample(nrow(Boston),nrow(Boston)*0.8)
boston.train<-Boston[subset,]
boston.test<-Boston[-subset,]
#Lasso Regression
lasso.model<-glmnet(bostontrain.mat,boston.train$medv,alpha=0,lambda=grid)
boston.cv.lasso<-cv.glmnet(bostontrain.mat,boston.train$medv,alpha=0,lambda=grid)
boston.bestlam.lasso<-boston.cv.lasso$lambda.min
boston.bestlam.lasso
boston.pred.lasso<-predict(lasso.model,s=boston.bestlam.lasso,newx=bostontest.mat)
#Mean Square Error calculation
lasso.MSE<-mean((boston.test$medv-boston.pred.lasso)^2)
lasso.MSE
#Principal Component Regression
pcr.model<-pcr(medv~.,data=boston.train,scale=TRUE,validation="CV")
pcr.model.7comp<-pcr(medv~.,data=boston.train,scale=TRUE,ncomp=7)
boston.predict.pcr<-predict(pcr.model,boston.test,ncomp=7)
pcr.MSE<-mean((boston.test$medv-boston.predict.pcr)^2)
pcr.MSE
#Partial Least Square
plsr.model<-plsr(medv~.,data=boston.train,scale=TRUE,validation="CV")
plsr.model.7comp<-plsr(medv~.,data=boston.train,scale=TRUE,ncomp=7)
boston.predict.plsr<-predict(plsr.model,boston.test,ncomp=7)
plsr.MSE<-mean((boston.test$medv-boston.predict.plsr)^2)
plsr.MSE
#Test train split
set.seed(2021)
subset<-sample(nrow(Boston),nrow(Boston)*0.8)
boston.train<-Boston[subset,]
boston.test<-Boston[-subset,]
#Lasso Regression
lasso.model<-glmnet(bostontrain.mat,boston.train$medv,alpha=0,lambda=grid)
boston.cv.lasso<-cv.glmnet(bostontrain.mat,boston.train$medv,alpha=0,lambda=grid)
boston.bestlam.lasso<-boston.cv.lasso$lambda.min
boston.bestlam.lasso
boston.pred.lasso<-predict(lasso.model,s=boston.bestlam.lasso,newx=bostontest.mat)
#Mean Square Error calculation
lasso.MSE<-mean((boston.test$medv-boston.pred.lasso)^2)
lasso.MSE
#Principal Component Regression
pcr.model<-pcr(medv~.,data=boston.train,scale=TRUE,validation="CV")
pcr.model.7comp<-pcr(medv~.,data=boston.train,scale=TRUE,ncomp=7)
boston.predict.pcr<-predict(pcr.model,boston.test,ncomp=7)
pcr.MSE<-mean((boston.test$medv-boston.predict.pcr)^2)
pcr.MSE
#Partial Least Square
plsr.model<-plsr(medv~.,data=boston.train,scale=TRUE,validation="CV")
plsr.model.7comp<-plsr(medv~.,data=boston.train,scale=TRUE,ncomp=7)
boston.predict.plsr<-predict(plsr.model,boston.test,ncomp=7)
plsr.MSE<-mean((boston.test$medv-boston.predict.plsr)^2)
plsr.MSE
#Test train split
set.seed(2021)
subset<-sample(nrow(Boston),nrow(Boston)*0.8)
boston.train<-Boston[subset,]
boston.test<-Boston[-subset,]
#Ridge Regression
#creating matrix for Ridge Regression
bostontrain.mat<-model.matrix(medv~.,data=boston.train)
bostontest.mat<-model.matrix(medv~.,data=boston.test)
grid<-10^seq(4,-2,length=100)
ridge.model<-glmnet(bostontrain.mat,boston.train$medv,alpha=0,lambda=grid)
boston.cv.ridge<-cv.glmnet(bostontrain.mat,boston.train$medv,alpha=0,lambda=grid)
boston.bestlam.ridge<-boston.cv.ridge$lambda.min
boston.bestlam.ridge
boston.pred.newridge<-predict(ridge.model,s=boston.bestlam.ridge,newx=bostontest.mat)
#Mean Square Error calculation
ridge.MSE<-mean((boston.test$medv-boston.pred.newridge)^2)
ridge.MSE
#Lasso Regression
lasso.model<-glmnet(bostontrain.mat,boston.train$medv,alpha=0,lambda=grid)
boston.cv.lasso<-cv.glmnet(bostontrain.mat,boston.train$medv,alpha=0,lambda=grid)
boston.bestlam.lasso<-boston.cv.lasso$lambda.min
boston.bestlam.lasso
boston.pred.lasso<-predict(lasso.model,s=boston.bestlam.lasso,newx=bostontest.mat)
#Mean Square Error calculation
lasso.MSE<-mean((boston.test$medv-boston.pred.lasso)^2)
lasso.MSE
#Principal Component Regression
pcr.model<-pcr(medv~.,data=boston.train,scale=TRUE,validation="CV")
pcr.model.7comp<-pcr(medv~.,data=boston.train,scale=TRUE,ncomp=7)
boston.predict.pcr<-predict(pcr.model,boston.test,ncomp=7)
pcr.MSE<-mean((boston.test$medv-boston.predict.pcr)^2)
pcr.MSE
#Partial Least Square
plsr.model<-plsr(medv~.,data=boston.train,scale=TRUE,validation="CV")
plsr.model.7comp<-plsr(medv~.,data=boston.train,scale=TRUE,ncomp=7)
boston.predict.plsr<-predict(plsr.model,boston.test,ncomp=7)
plsr.MSE<-mean((boston.test$medv-boston.predict.plsr)^2)
plsr.MSE
p = seq(0, 1, 0.01)
gini = p * (1 - p) * 2
entropy = -(p * log(p) + (1 - p) * log(1 - p))
class.err = 1 - pmax(p, 1 - p)
matplot(p, cbind(gini, entropy, class.err), type = "l", col = c("red", "green", "blue"))
legend("topright",legend=c("gini index","entropy", "classification error"),pch=3,col=c("red", "green", "blue"))
p = seq(0, 1, 0.01)
gini = p * (1 - p) * 2
entropy = -(p * log(p) + (1 - p) * log(1 - p))
class.err = 1 - pmax(p, 1 - p)
matplot(p, cbind(gini, entropy, class.err), ylab='error rate' type = "l", col = c("red", "green", "blue"))
legend("topright",legend=c("gini index","entropy", "classification error"),col=c("black", "red', "blue"))
p = seq(0, 1, 0.01)
gini = p * (1 - p) * 2
entropy = -(p * log(p) + (1 - p) * log(1 - p))
class.err = 1 - pmax(p, 1 - p)
matplot(p, cbind(gini, entropy, class.err), ylab='error rate', type = "l", col = c("red", "green", "blue"))
legend("topright",legend=c("gini index","entropy", "classification error"),col=c("black", "red', "blue"))
p = seq(0, 1, 0.01)
gini = p * (1 - p) * 2
entropy = -(p * log(p) + (1 - p) * log(1 - p))
class.err = 1 - pmax(p, 1 - p)
matplot(p, cbind(gini, entropy, class.err), ylab='error rate', type = "l", col = c("red", "green", "blue"))
legend("topright",legend=c("gini index","entropy", "classification error"),col=c("red', "blue"))
p = seq(0, 1, 0.01)
gini = p * (1 - p) * 2
entropy = -(p * log(p) + (1 - p) * log(1 - p))
class.err = 1 - pmax(p, 1 - p)
matplot(p, cbind(gini, entropy, class.err), ylab='error rate', type = "l", col = c("red", "green", "blue"))
legend("topright",legend=c("gini index","entropy", "classification error"),col=c("red',"blue"))
p = seq(0, 1, 0.01)
gini = p * (1 - p) * 2
entropy = -(p * log(p) + (1 - p) * log(1 - p))
class.err = 1 - pmax(p, 1 - p)
matplot(p, cbind(gini, entropy, class.err), ylab="error rate", type = "l", col = c("red", "green", "blue"))
legend("topright",legend=c("gini index","entropy", "classification error"),col=c("red',"blue"))
p = seq(0, 1, 0.01)
gini = p * (1 - p) * 2
entropy = -(p * log(p) + (1 - p) * log(1 - p))
class.err = 1 - pmax(p, 1 - p)
matplot(p, cbind(gini, entropy, class.err), type = "l", col = c("red", "green", "blue"))
legend("topright",legend=c("gini index","entropy", "classification error"),pch=19,col=c("red", "green", "blue"))
p = seq(0, 1, 0.01)
gini = p * (1 - p) * 2
entropy = -(p * log(p) + (1 - p) * log(1 - p))
class.err = 1 - pmax(p, 1 - p)
matplot(p, cbind(gini, entropy, class.err), type = "l", col = c("red", "green", "blue"))
legend("topright",legend=c("gini index","entropy", "classification error"),pch=19,col=c("black", "red", "blue"))
p = seq(0, 1, 0.01)
gini = p * (1 - p) * 2
entropy = -(p * log(p) + (1 - p) * log(1 - p))
class.err = 1 - pmax(p, 1 - p)
matplot(p, cbind(gini, entropy, class.err), ylab="error rate", type = "l", col = c("black", "red", "blue"))
legend("topright",legend=c("gini index","entropy", "classification error"),pch=19,col=c("black", "red", "blue"))
library(ISLR)
set.seed(2021)
train = sample(dim(Carseats)[1], dim(Carseats)[1]/2)
Carseats.train = Carseats[train, ]
Carseats.test = Carseats[-train, ]
library(ISLR2)
attach(Carseats)
set.seed(2021)
train = sample(dim(Carseats)[1], dim(Carseats)[1]/2)
Carseats.train = Carseats[train, ]
Carseats.test = Carseats[-train, ]
library(tree)
tree.carseats = tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
install.packages(tree)
library(tree)
install.packages("tree")
R.Version()
tree.carseats = tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
library(tree)
tree.carseats = tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
plot(tree.carseats)
text(tree.carseats, pretty = 1)
plot(tree.carseats)
text(tree.carseats, pretty = 4)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
plot(tree.carseats)
text(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty=0)
pred.carseats = predict(tree.carseats, Carseats.test)
mean((Carseats.test$Sales - pred.carseats)^2)
cv.carseats = cv.tree(tree.carseats, FUN=prune.tree)
cv.carseats$dev
cv.carseats = cv.tree(tree.carseats, FUN=prune.tree)
min(cv.carseats$dev)
which.min(cv.carseats$dev)
pruned.carseats = prune.tree(tree.carseats, best = 5)
par(mfrow = c(1, 1))
plot(pruned.carseats)
text(pruned.carseats, pretty = 0)
pred.pruned = predict(pruned.carseats, Carseats.test)
mean((Carseats.test$Sales - pred.pruned)^2)
install.packages("randomForest")
library(randomForest)
set.seed(2021)
bag.carseats = randomForest(Sales~., data=Carseats.train, mtry = 10, ntree = 500, importance=TRUE)
bag.carseats
bag.pred = predict(bag.carseats, Carseats.test)
mean((Carseats.test$Sales - bag.pred)^2)
importance(bag.carseats)
oob.err=double(10)
test.err=double(10)
for(mtry in 1:10){
rf.carseats = randomForest(Sales ~ ., data = Carseats.train, mtry = mtry, ntree = 500,
importance = T)
oob.err[mtry]=rf.carseats$mse[500]
rf.pred = predict(rf.carseats, Carseats.test)
test.err[mtry] = mean((Carseats.test$Sales - rf.pred)^2)
cat(mtry," ")
}
test.err[which.min(test.err)]
importance(rf.carseats)
install.packages("gbm")
library(ISLR2)
Hitters = Hitters[-which(is.na(Hitters$Salary)), ]
Hitters$Salary = log(Hitters$Salary)
train = 1:200
Hitters.train = Hitters[train, ]
Hitters.test = Hitters[-train, ]
library(gbm)
set.seed(2021)
pows = seq(-10, -0.2, by=0.1)
lambdas = 10 ^ pows
train.errors = rep(NA, length(lambdas))
test.errors = rep(NA, length(lambdas))
for(i in 1:length(lambdas)){
boost.hitters = gbm(Salary~., data=Hitters.train, distribution="gaussian", n.trees=1000, shrinkage=lambdas[i])
train.pred = predict(boost.hitters, Hitters.train, n.trees=1000)
test.pred = predict(boost.hitters, Hitters.test, n.trees=1000)
train.errors[i] = mean((Hitters.train$Salary - train.pred)^2)
test.errors[i] = mean((Hitters.test$Salary - test.pred)^2)
}
plot(lambdas, train.errors, type="b", xlab="Shrinkage values", ylab="Train MSE", col="blue", pch=20)
plot(lambdas, test.errors, type="b", xlab="Shrinkage values", ylab="Test MSE", col="blue", pch=20)
library(ISLR2)
train = 1:1000
Caravan$Purchase = ifelse(Caravan$Purchase == "Yes", 1, 0)
Caravan.train = Caravan[train, ]
Caravan.test = Caravan[-train, ]
library(gbm)
set.seed(2021)
boost.caravan = gbm(Purchase~., data=Caravan.train, n.trees = 1000, shrinkage = 0.01, distribution = "bernoulli")
summary(boost.caravan)
boost.prob = predict(boost.caravan, Caravan.test, n.trees = 1000, type = "response")
boost.pred = ifelse(boost.prob > 0.2, 1, 0)
table(Caravan.test$Purchase, boost.pred)
34/(120+34)
d = as.dist(matrix(c(0, 0.3, 0.4, 0.7,
0.3, 0, 0.5, 0.8,
0.4, 0.5, 0.0, 0.45,
0.7, 0.8, 0.45, 0.0), nrow = 4))
plot(hclust(d, method = "complete"))
plot(hclust(d, method = "single"))
plot(hclust(d, method = "complete"), labels = c(1,4,2,3))
plot(hclust(d, method = "complete"), labels = c(2,1,3,4))
library(ISLR)
set.seed(1)
dsc <- scale(USArrests)
d1 <- dist(dsc)^2
d2 <- as.dist(1 - cor(t(dsc)))
summary(d2 / d1)
library(ISLR2)
set.seed(1)
dsc <- scale(USArrests)
d1 <- dist(dsc)^2
d2 <- as.dist(1 - cor(t(dsc)))
summary(d2 / d1)
library(ISLR2)
set.seed(2021)
dsc <- scale(USArrests)
d1 <- dist(dsc)^2
d2 <- as.dist(1 - cor(t(dsc)))
summary(d2 / d1)
library(ISLR2)
set.seed(2021)
dsc <- scale(USArrests)
d1 <- dist(dsc)^2
d2 <- as.dist(1 - cor(t(dsc)))
summary(abs(d2-d1))
d1
d1-d2
library(ISLR2)
set.seed(2021)
dsc <- scale(USArrests)
d1 <- dist(dsc)^2
d2 <- as.dist(1 - cor(t(dsc)))
summary(abs(d2-d1))
library(ISLR2)
set.seed(2021)
dsc <- scale(USArrests)
d1 <- dist(dsc)^2
d2 <- as.dist(1 - cor(t(dsc)))
summary(d2 / d1)
d2/d1
library(ISLR2)
set.seed(2021)
dsc <- scale(USArrests)
d1 <- dist(dsc)^2
d2 <- as.dist(1 - cor(t(dsc)))
summary(d2 / d1)
pr.out <- prcomp(USArrests, scale = TRUE)
pr.var <- pr.out$sdev^2
pve <- pr.var / sum(pr.var)
sum(pr.var)
pve
loadings <- pr.out$rotation
USArrests2 <- scale(USArrests)
sumvar <- sum(apply(as.matrix(USArrests2)^2, 2, sum))
apply((as.matrix(USArrests2) %*% loadings)^2, 2, sum) / sumvar
set.seed(2021)
hc.complete <- hclust(dist(USArrests), method = "complete")
plot(hc.complete)
cutree(hc.complete, 3)
sd.data <- scale(USArrests)
hc.complete.sd <- hclust(dist(sd.data), method = "complete")
plot(hc.complete.sd)
cutree(hc.complete.sd, 3)
table(cutree(hc.complete, 3), cutree(hc.complete.sd, 3))
load("C:/Users/liang/OneDrive/Desktop/NEAT/laws/R/Results/SNPs_de.Rdata")
View(snps_de)
load("C:/Users/liang/OneDrive/Desktop/NEAT/laws/R/Results/SNPs_rej.Rdata")
View(snps_rejection)
View(snps_rejection)
load("C:/Users/liang/OneDrive/Desktop/NEAT/laws/R/Results/SNPs_nr.Rdata")
par(mfrow=c(2, 2), mgp=c(2, 0.5, 0), mar=c(3, 3, 2, 1)+0.1)
# matplot(noise_l.vec, fdr_tf1.mthd, type="o", pch=1:4, lwd=2, main="TF-setting1 FDR Comparison", xlab=TeX('$\\sigma_s$'), ylab="FDR", ylim=c(0.01, 0.10))
# legend("top", c("BH", "NEAT.OR","NEAT.DD"), pch=1:3, col=1:6, lwd=2)
#
# matplot(noise_l.vec, etp_tf1.mthd, type="o", pch=1:4, lwd=2, main="TF-setting1 Power Comparison", xlab=TeX('$\\sigma_s$'), ylab="ETP")
matplot(noise_l.vec, fdr_tf1.mthd, type="o", pch=1:4, lwd=2, main="TF-setting1 FDR Comparison", xlab=expression(sigma), ylab="FDR", ylim=c(0, 0.09))
matplot(noise_l.vec, etp_tf1.mthd, type="o", pch=1:4, lwd=2, main="TF-setting1 Power Comparison", xlab=expression(sigma), ylab="ETP")
legend("topright", c("BH", "LATLA.OR","LATLA.DD", "SABHA", "WBH"), pch=1:3, col=1:6, lwd=2)
m<-1200
noise_l.vec<-seq(from=0.5, to=2, by=0.25)
?load
getwd
getwd()
load(file='C:/Users/liang/OneDrive/Desktop/LATLA/laws/R/Results/fdr_tf1.Rdata')
load(file='C:/Users/liang/OneDrive/Desktop/LATLA/laws/R/Results/fdr_tf2.Rdata')
load(file='C:/Users/liang/OneDrive/Desktop/LATLA/laws/R/Results/etp_tf2.Rdata')
load(file='C:/Users/liang/OneDrive/Desktop/LATLA/laws/R/Results/etp_tf1.Rdata')
par(mfrow=c(2, 2), mgp=c(2, 0.5, 0), mar=c(3, 3, 2, 1)+0.1)
# matplot(noise_l.vec, fdr_tf1.mthd, type="o", pch=1:4, lwd=2, main="TF-setting1 FDR Comparison", xlab=TeX('$\\sigma_s$'), ylab="FDR", ylim=c(0.01, 0.10))
# legend("top", c("BH", "NEAT.OR","NEAT.DD"), pch=1:3, col=1:6, lwd=2)
#
# matplot(noise_l.vec, etp_tf1.mthd, type="o", pch=1:4, lwd=2, main="TF-setting1 Power Comparison", xlab=TeX('$\\sigma_s$'), ylab="ETP")
matplot(noise_l.vec, fdr_tf1.mthd, type="o", pch=1:4, lwd=2, main="TF-setting1 FDR Comparison", xlab=expression(sigma), ylab="FDR", ylim=c(0, 0.09))
matplot(noise_l.vec, etp_tf1.mthd, type="o", pch=1:4, lwd=2, main="TF-setting1 Power Comparison", xlab=expression(sigma), ylab="ETP")
legend("topright", c("BH", "LATLA.OR","LATLA.DD", "SABHA", "WBH"), pch=1:3, col=1:6, lwd=2)
?matplot
par(mfrow=c(2, 2), mgp=c(2, 0.5, 0), mar=c(3, 3, 2, 1)+0.1)
# matplot(noise_l.vec, fdr_tf1.mthd, type="o", pch=1:4, lwd=2, main="TF-setting1 FDR Comparison", xlab=TeX('$\\sigma_s$'), ylab="FDR", ylim=c(0.01, 0.10))
# legend("top", c("BH", "NEAT.OR","NEAT.DD"), pch=1:3, col=1:6, lwd=2)
#
# matplot(noise_l.vec, etp_tf1.mthd, type="o", pch=1:4, lwd=2, main="TF-setting1 Power Comparison", xlab=TeX('$\\sigma_s$'), ylab="ETP")
matplot(noise_l.vec, fdr_tf1.mthd, type="o", pch=1:4, lwd=2, main="TF-setting1 FDR Comparison", xlab=expression(sigma), ylab="FDR", ylim=c(0, 0.09))
matplot(noise_l.vec, etp_tf1.mthd, type="o", pch=1:4, lwd=2, main="TF-setting1 Power Comparison", xlab=expression(sigma), ylab="ETP")
legend("topright", c("BH", "LATLA.OR","LATLA.DD", "SABHA", "WBH"), pch=1:4, col=1:6, lwd=2)
m<-1200
mu_l.vec<-seq(from=3, to=4, by=0.20)
noise<-1
matplot(mu_l.vec, fdr_tf2.mthd, type="o", pch=1:4, lwd=2, main="TF-setting2 FDR Comparison", xlab=expression(mu), ylab="FDR", ylim=c(0.01, 0.10))
legend("top", c("BH", "LATLA.OR","LATLA.DD", "SABHA", "WBH"), pch=1:4, col=1:6, lwd=2)
matplot(mu_l.vec, etp_tf2.mthd, type="o", pch=1:4, lwd=2, main="TF-setting2 Power Comparison", xlab=expression(mu), ylab="ETP")
setwd('./LATLA')
devtools::document()
devtools::check()
devtools::check()
devtools::check()
devtools::check()
devtools::document()
devtools::document()
devtools::check()
devtools::document()
devtools::check()
devtools::document
devtools::document()
library(stats)
usethis::use_package("stats")
?requireNamespace
search()
devtools::document()
devtools::document()
warnings()
devtools::document()
warnings()
devtools::document()
warnings()
devtools::document()
version
Sys.getenv("R_ARCH")
qpdf
qpdf
qpdf
qpdf
qpdf
qpdf
use_mit_license()
use_mit_license(copyright_holder = NULL)
usethis::use_mit_license()
rhub::check_for_cran()
devtools::check_rhub()
devtools::check_win_devel()
devtools::spell_check()
devtools::spell_check()
devtools::document()
devtools::spell_check()
devtools::document()
devtools::spell_check()
goodpractice::gp()
install.packages('goodpractice')
goodpractice::gp()
devtools::document()
?latla_pis
?latla_weights
goodpractice::gp()
